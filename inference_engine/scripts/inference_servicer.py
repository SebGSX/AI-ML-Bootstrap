# Â© 2023 Seb Garrioch. All rights reserved.
# Published under the MIT License.
import grpc
import inference_engine_pb2
import inference_engine_pb2_grpc
import signal
from grpc import Server
from torch import FloatTensor, LongTensor
from transformers import BatchEncoding, PreTrainedModel, PreTrainedTokenizer, StoppingCriteria, StoppingCriteriaList
from typing import List


class ModelConfiguration:
    """Represents a model's configuration."""

    AI_ID: str = "<bot>"
    """A string representing the token used to identify an AI participant."""

    HUMAN_ID: str = "<human>"
    """A string representing the token used to identify a human participant."""

    MODEL_NAME: str = "togethercomputer/GPT-NeoXT-Chat-Base-20B"
    """A string constant representing the model's name."""

    cache_dir: str = None
    """A string representing the name and location of the cache directory used to cache model data."""

    def __init__(self, cache_dir: str):
        """Initializes a model configuration.
        :param cache_dir: A string representing the name and location of the cache directory used to cache model data.
        """

        if not cache_dir:
            raise ValueError("The cache_dir parameter must contain a value.")

        self.cache_dir = cache_dir


class StoppingWordsCriteria(StoppingCriteria):
    """Represents a words-based stopping criteria that can be applied during generation."""

    def __init__(self, tokenizer: PreTrainedTokenizer, stopping_words: List[str]):
        """Initializes a new instance of the StoppingWordsCriteria class.
        :param tokenizer: A PreTrainedTokenizer representing a model's tokenizer.
        :param stopping_words: A List[str] representing the list of stopping words that are the basis of the criteria.
        """

        self._tokenizer: PreTrainedTokenizer = tokenizer
        self._stopping_words: List[str] = stopping_words
        self._text_stream: str = ""

    def __call__(self, input_ids: LongTensor, scores: FloatTensor, **kwargs: dict[str, any]) -> bool:
        """Checks the input IDs for stopping words.
        :param input_ids: A LongTensor representing the input IDs to check for stopping words.
        :param scores: A FloatTensor representing the scores.
        :param kwargs: A dict[str, any] representing a dictionary of keyword arguments.
        :returns: A boolean indicating whether a stopping word was found in the input IDs.
        """

        self._text_stream += self._tokenizer.decode(input_ids[0, -1])
        for stopping_word in self._stopping_words:
            if stopping_word in self._text_stream:
                return True

        return False


class InferenceServicer(inference_engine_pb2_grpc.InferenceServiceServicer):
    """Represents an inference servicer."""

    _config: ModelConfiguration = None
    _grpc_server: Server = None
    _model: PreTrainedModel = None
    _tokenizer: PreTrainedTokenizer = None

    is_running: bool = False
    """A boolean indicating whether the servicer is running."""

    def __init__(
            self,
            model: PreTrainedModel,
            tokenizer: PreTrainedTokenizer,
            config: ModelConfiguration,
            grpc_server: Server):
        """Initializes a new instance of the InferenceServicer class.
        :param model: A PreTrainedModel representing the machine learning model with which to conduct inference.
        :param tokenizer: A PreTrainedTokenizer representing the model's associated tokenizer.
        :param config: A ModelConfiguration representing the model's configuration.
        :param grpc_server: A Server representing the gRPC server.
        """

        self._config = config
        self._grpc_server = grpc_server
        self._model = model
        self._tokenizer = tokenizer

    def GetResponse(self, request: inference_engine_pb2.Request, context: grpc.ServicerContext) \
            -> inference_engine_pb2.Response:
        """Gets a response generated by means of inference from the active model.
        :param request: A Request representing a request for the inference engine.
        :param context: A ServicerContext representing the gRPC context used to handle the request.
        :return: A Response representing a response from the inference engine.
        """

        if not self.is_running:
            context.abort(grpc.StatusCode.UNAVAILABLE, "The inference servicer is not running.")
            return inference_engine_pb2.Response()

        request_text: str = f"{self._config.HUMAN_ID}: {request.text}\n{self._config.AI_ID}:"
        inputs: BatchEncoding = self._tokenizer(
            request_text,
            return_tensors="pt").to(self._model.device)

        outputs = self._model.generate(
            **inputs,
            max_new_tokens=100,
            do_sample=False,
            temperature=0.8,
            pad_token_id=self._tokenizer.eos_token_id,
            stopping_criteria=StoppingCriteriaList([StoppingWordsCriteria(
                self._tokenizer,
                [self._config.HUMAN_ID])]))

        response: str = self._tokenizer.decode(outputs[0], skip_special_ids=True)
        response = response.replace(request_text + ' ', '')
        response = response.replace(self._config.HUMAN_ID + ':', '')

        return inference_engine_pb2.Response(text=response)

    def run(self, port: int):
        """Runs the inference servicer.
        :param port: An integer representing the port number to use for the gRPC server.
        """

        if 1024 > port or port > 65535:
            raise ValueError("The port parameter must be greater than 1,023 and less than 65,536")

        self._grpc_server.add_insecure_port(f"[::]:{port}")
        inference_engine_pb2_grpc.add_InferenceServiceServicer_to_server(
            self,
            self._grpc_server)

        def shutdown_signal_handler(*_):
            """Handles shutdown signals.
            :param _: Ignore all parameters.
            """

            print("Shutdown signal received.")
            self.shutdown()

        print('Adding signal handlers...')
        signal.signal(signal.SIGINT, shutdown_signal_handler)
        signal.signal(signal.SIGTERM, shutdown_signal_handler)

        print("Server starting...")
        self._grpc_server.start()

        self.is_running = True

        print("Server started. Press Ctrl+C to stop.")
        self._grpc_server.wait_for_termination()

    def shutdown(self):
        """Gracefully shuts down the inference servicer."""

        print("Shutting down the inference servicer and saving changes to the inference model...")
        self.is_running = False
        self._grpc_server.stop(0)
        print("Shutdown successful.")
